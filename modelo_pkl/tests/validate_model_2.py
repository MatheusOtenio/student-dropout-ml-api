#!/usr/bin/env python3
"""
Script para validar a qualidade de um modelo treinado.
Uso: python validate_model.py [caminho_do_modelo.pkl]
"""

import sys
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

import joblib
import numpy as np
import pandas as pd
from termcolor import colored

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

# Adiciona o diret√≥rio raiz ao path
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))


def load_model(path: str) -> Dict[str, Any]:
    """Carrega o modelo do arquivo pkl."""
    try:
        return joblib.load(path)
    except Exception as e:
        raise ValueError(f"Erro ao carregar modelo: {e}")


def interpret_roc_auc(score: float) -> Tuple[str, str]:
    """Interpreta o score ROC-AUC."""
    if score >= 0.90:
        return ("EXCEPCIONAL", "green")
    elif score >= 0.85:
        return ("EXCELENTE", "green")
    elif score >= 0.75:
        return ("BOM", "blue")
    elif score >= 0.65:
        return ("RAZO√ÅVEL", "yellow")
    elif score >= 0.55:
        return ("FRACO", "yellow")
    else:
        return ("MUITO FRACO", "red")


def interpret_brier(score: float) -> Tuple[str, str]:
    """Interpreta o Brier Score."""
    if score <= 0.08:
        return ("EXCEPCIONAL", "green")
    elif score <= 0.12:
        return ("EXCELENTE", "green")
    elif score <= 0.15:
        return ("BOM", "blue")
    elif score <= 0.20:
        return ("RAZO√ÅVEL", "yellow")
    elif score <= 0.25:
        return ("FRACO", "yellow")
    else:
        return ("MUITO FRACO", "red")


def get_recommendations(metadata: Dict[str, Any]) -> List[str]:
    """Gera recomenda√ß√µes baseadas nos metadados do modelo."""
    recommendations = []
    
    n_samples = metadata.get("n_samples", 0)
    n_features = metadata.get("n_features", 0)
    metrics = metadata.get("metrics", {})
    roc_auc = metrics.get("roc_auc", 0) or 0.0
    brier = metrics.get("brier_score", 1) or 1.0
    model_type = metadata.get("model_type", "unknown")

    # An√°lise de tamanho do dataset
    if n_samples < 300:
        recommendations.append(
            "üî¥ CR√çTICO: Dataset muito pequeno (<300 amostras).\n"
            "   ‚Ä¢ Coletar pelo menos 500-1000 amostras para resultados confi√°veis\n"
            "   ‚Ä¢ Considere usar valida√ß√£o leave-one-out ao inv√©s de k-fold\n"
            "   ‚Ä¢ Modelo pode estar overfitting severamente"
        )
    elif n_samples < 500:
        recommendations.append(
            "‚ö†Ô∏è  Dataset pequeno (<500 amostras).\n"
            "   ‚Ä¢ Coletar mais dados melhorar√° significativamente o modelo\n"
            "   ‚Ä¢ Evite modelos muito complexos (use regulariza√ß√£o forte)"
        )
    elif n_samples < 1000:
        recommendations.append(
            "üí° Dataset razo√°vel, mas mais dados sempre ajudam.\n"
            "   ‚Ä¢ Meta: 1000+ amostras para modelos mais robustos"
        )

    # An√°lise de performance
    if roc_auc < 0.55:
        recommendations.append(
            "üî¥ CR√çTICO: ROC-AUC muito pr√≥ximo do aleat√≥rio (0.50).\n"
            "   ‚Ä¢ Verificar se as features t√™m poder preditivo\n"
            "   ‚Ä¢ Revisar se o target est√° corretamente mapeado\n"
            "   ‚Ä¢ Checar balanceamento de classes\n"
            "   ‚Ä¢ Considere feature selection/engineering profundo"
        )
    elif roc_auc < 0.70:
        recommendations.append(
            "üìä ROC-AUC abaixo do ideal. Considere:\n"
            "   ‚Ä¢ Feature engineering (criar novas features relevantes)\n"
            "   ‚Ä¢ An√°lise de correla√ß√£o entre features e target\n"
            "   ‚Ä¢ Remover features ruidosas\n"
            "   ‚Ä¢ Otimiza√ß√£o de hiperpar√¢metros\n"
            "   ‚Ä¢ Experimentar outros algoritmos (XGBoost, CatBoost)"
        )
    elif roc_auc < 0.80:
        recommendations.append(
            "‚úÖ ROC-AUC bom, mas h√° espa√ßo para melhoria:\n"
            "   ‚Ä¢ Fine-tuning de hiperpar√¢metros\n"
            "   ‚Ä¢ Ensemble de modelos\n"
            "   ‚Ä¢ Feature engineering avan√ßado"
        )

    if brier > 0.20:
        recommendations.append(
            "üéØ Brier Score alto indica m√° calibra√ß√£o. Considere:\n"
            "   ‚Ä¢ Aplicar calibra√ß√£o (Platt Scaling ou Isotonic Regression)\n"
            "   ‚Ä¢ Revisar dados de treino/valida√ß√£o\n"
            "   ‚Ä¢ Verificar se h√° outliers nos dados\n"
            "   ‚Ä¢ Ajustar threshold de decis√£o baseado em custo-benef√≠cio"
        )
    elif brier > 0.15:
        recommendations.append(
            "‚öñÔ∏è  Calibra√ß√£o pode ser melhorada:\n"
            "   ‚Ä¢ Testar diferentes m√©todos de calibra√ß√£o\n"
            "   ‚Ä¢ Verificar distribui√ß√£o das probabilidades preditas"
        )

    # An√°lise de features
    if n_features > 100 and n_samples < 1000:
        recommendations.append(
            "‚ö†Ô∏è  Raz√£o features/amostras desfavor√°vel.\n"
            "   ‚Ä¢ Considere feature selection (remove features irrelevantes)\n"
            "   ‚Ä¢ Use regulariza√ß√£o forte (L1 ou ElasticNet)\n"
            "   ‚Ä¢ Aplique PCA/dimensionality reduction se apropriado"
        )
    elif n_features > 50 and n_samples < 500:
        recommendations.append(
            "üí° Muitas features para poucos dados:\n"
            "   ‚Ä¢ Feature selection pode melhorar generaliza√ß√£o\n"
            "   ‚Ä¢ Use regulariza√ß√£o para prevenir overfitting"
        )

    # An√°lise de balanceamento
    class_mapping = metadata.get("class_mapping", {})
    if class_mapping:
        recommendations.append(
            "‚öñÔ∏è  Verificar balanceamento de classes:\n"
            "   ‚Ä¢ Se muito desbalanceado (>80/20), considere:\n"
            "     - SMOTE ou outras t√©cnicas de oversampling\n"
            "     - Ajustar class_weight no modelo\n"
            "     - Usar m√©tricas apropriadas (F1, Precision-Recall AUC)\n"
            "   ‚Ä¢ Se balanceado, est√° ok!"
        )

    # Recomenda√ß√µes por tipo de modelo
    if model_type == "lightgbm":
        if roc_auc < 0.80:
            recommendations.append(
                "üå≥ LightGBM espec√≠fico:\n"
                "   ‚Ä¢ Ajustar num_leaves e max_depth\n"
                "   ‚Ä¢ Testar diferentes learning_rates\n"
                "   ‚Ä¢ Experimentar min_child_samples para evitar overfitting"
            )
    elif model_type == "logreg":
        if roc_auc < 0.75:
            recommendations.append(
                "üìà Regress√£o Log√≠stica:\n"
                "   ‚Ä¢ Pode ser muito simples para este problema\n"
                "   ‚Ä¢ Considere modelos n√£o-lineares (LightGBM, XGBoost)\n"
                "   ‚Ä¢ Adicione features polinomiais ou intera√ß√µes"
            )

    return recommendations


def print_header(text: str):
    """Imprime cabe√ßalho formatado."""
    print("\n" + "=" * 80)
    print(colored(f"  {text}", "cyan", attrs=["bold"]))
    print("=" * 80)


def print_metric(name: str, value: float, interpretation: str, color: str, info: str = ""):
    """Imprime m√©trica formatada."""
    print(f"\n{colored(name + ':', 'white', attrs=['bold'])} {value:.4f}")
    print(f"  ‚îú‚îÄ Avalia√ß√£o: {colored(interpretation, color, attrs=['bold'])}")
    if info:
        print(f"  ‚îî‚îÄ Info: {info}")


def calculate_additional_metrics(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """Calcula m√©tricas adicionais √∫teis."""
    additional = {}
    
    n_samples = metadata.get("n_samples", 0)
    n_features = metadata.get("n_features", 0)
    
    if n_samples > 0 and n_features > 0:
        additional["samples_per_feature"] = n_samples / n_features
        
        # Regra de ouro: 10+ samples por feature
        if additional["samples_per_feature"] >= 10:
            additional["data_sufficiency"] = "ADEQUADO"
            additional["sufficiency_color"] = "green"
        elif additional["samples_per_feature"] >= 5:
            additional["data_sufficiency"] = "ACEIT√ÅVEL"
            additional["sufficiency_color"] = "yellow"
        else:
            additional["data_sufficiency"] = "INSUFICIENTE"
            additional["sufficiency_color"] = "red"
    
    return additional


def compare_with_baseline(metadata: Dict[str, Any]):
    """Compara o modelo com baseline aleat√≥rio."""
    metrics = metadata.get("metrics", {})
    roc_auc = metrics.get("roc_auc", 0) or 0.0
    brier = metrics.get("brier_score", 1) or 1.0
    
    baseline_roc = 0.50
    baseline_brier = 0.25
    
    improvement_roc = ((roc_auc - baseline_roc) / baseline_roc) * 100
    improvement_brier = ((baseline_brier - brier) / baseline_brier) * 100
    
    print_header("üìà COMPARA√á√ÉO COM BASELINE")
    
    print(f"\n{'ROC-AUC:':<30}")
    print(f"  {'Modelo Atual:':<25} {colored(f'{roc_auc:.4f}', 'cyan')}")
    print(f"  {'Baseline (aleat√≥rio):':<25} {baseline_roc:.4f}")
    
    if improvement_roc > 50:
        color_roc = "green"
        emoji_roc = "üéØ"
    elif improvement_roc > 20:
        color_roc = "blue"
        emoji_roc = "‚úì"
    elif improvement_roc > 0:
        color_roc = "yellow"
        emoji_roc = "‚ö†Ô∏è"
    else:
        color_roc = "red"
        emoji_roc = "‚ùå"
    
    print(f"  {'Melhoria:':<25} {colored(f'{emoji_roc} +{improvement_roc:.1f}%', color_roc, attrs=['bold'])}")
    
    print(f"\n{'Brier Score:':<30}")
    print(f"  {'Modelo Atual:':<25} {colored(f'{brier:.4f}', 'cyan')}")
    print(f"  {'Baseline (aleat√≥rio):':<25} {baseline_brier:.4f}")
    
    if improvement_brier > 40:
        color_brier = "green"
        emoji_brier = "üéØ"
    elif improvement_brier > 20:
        color_brier = "blue"
        emoji_brier = "‚úì"
    elif improvement_brier > 0:
        color_brier = "yellow"
        emoji_brier = "‚ö†Ô∏è"
    else:
        color_brier = "red"
        emoji_brier = "‚ùå"
    
    print(f"  {'Melhoria:':<25} {colored(f'{emoji_brier} +{improvement_brier:.1f}%', color_brier, attrs=['bold'])}")
    
    if improvement_roc < 10:
        print(colored("\n‚ùå ALERTA: Modelo marginalmente melhor que baseline!", "red", attrs=["bold"]))
        print("   ‚Ä¢ Revisar completamente a estrat√©gia de modelagem")
        print("   ‚Ä¢ Verificar qualidade e relev√¢ncia dos dados")
        print("   ‚Ä¢ Considere se o problema √© realmente previs√≠vel")


def plot_metrics_history(artifacts_dir: Path):
    """Plota hist√≥rico de m√©tricas dos modelos."""
    if not HAS_MATPLOTLIB:
        return
    
    from datetime import datetime
    
    models = sorted(artifacts_dir.glob("*.pkl"), key=lambda p: p.stat().st_mtime)
    if len(models) < 2:
        print("\nüí° Apenas um modelo encontrado. Hist√≥rico ser√° gerado com mais treinos.")
        return
    
    history = []
    for model_path in models:
        try:
            artifact = load_model(str(model_path))
            metadata = artifact.get("metadata", {})
            metrics = metadata.get("metrics", {})
            timestamp_str = metadata.get("timestamp", "")
            
            if not timestamp_str:
                continue
            
            timestamp = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
            
            history.append({
                "timestamp": timestamp,
                "roc_auc": metrics.get("roc_auc", 0) or 0.0,
                "brier": metrics.get("brier_score", 0) or 0.0,
                "name": model_path.name,
                "n_samples": metadata.get("n_samples", 0),
            })
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao processar {model_path.name}: {e}")
            continue
    
    if len(history) < 2:
        return
    
    # Configurar estilo
    sns.set_style("whitegrid")
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    
    timestamps = [h["timestamp"] for h in history]
    roc_values = [h["roc_auc"] for h in history]
    brier_values = [h["brier"] for h in history]
    sample_counts = [h["n_samples"] for h in history]
    
    # ROC-AUC ao longo do tempo
    axes[0, 0].plot(timestamps, roc_values, marker="o", linewidth=2, markersize=8)
    axes[0, 0].axhline(y=0.75, color="green", linestyle="--", alpha=0.5, label="Bom (0.75)")
    axes[0, 0].axhline(y=0.85, color="darkgreen", linestyle="--", alpha=0.5, label="Excelente (0.85)")
    axes[0, 0].axhline(y=0.50, color="red", linestyle="--", alpha=0.5, label="Baseline (0.50)")
    axes[0, 0].set_title("ROC-AUC ao Longo do Tempo", fontsize=12, fontweight="bold")
    axes[0, 0].set_ylabel("ROC-AUC Score")
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_ylim([0.4, 1.0])
    
    # Brier Score ao longo do tempo
    axes[0, 1].plot(timestamps, brier_values, marker="o", linewidth=2, markersize=8, color="orange")
    axes[0, 1].axhline(y=0.15, color="green", linestyle="--", alpha=0.5, label="Bom (0.15)")
    axes[0, 1].axhline(y=0.12, color="darkgreen", linestyle="--", alpha=0.5, label="Excelente (0.12)")
    axes[0, 1].axhline(y=0.25, color="red", linestyle="--", alpha=0.5, label="Baseline (0.25)")
    axes[0, 1].set_title("Brier Score ao Longo do Tempo", fontsize=12, fontweight="bold")
    axes[0, 1].set_ylabel("Brier Score (menor √© melhor)")
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Tamanho do dataset
    axes[1, 0].plot(timestamps, sample_counts, marker="s", linewidth=2, markersize=8, color="purple")
    axes[1, 0].set_title("Tamanho do Dataset de Treino", fontsize=12, fontweight="bold")
    axes[1, 0].set_ylabel("N√∫mero de Amostras")
    axes[1, 0].grid(True, alpha=0.3)
    
    # Melhoria vs Baseline
    improvements = [((roc - 0.5) / 0.5) * 100 for roc in roc_values]
    colors = ["green" if imp > 50 else "yellow" if imp > 20 else "red" for imp in improvements]
    axes[1, 1].bar(range(len(improvements)), improvements, color=colors, alpha=0.7)
    axes[1, 1].axhline(y=20, color="blue", linestyle="--", alpha=0.5, label="M√≠nimo Aceit√°vel")
    axes[1, 1].set_title("Melhoria vs Baseline (%)", fontsize=12, fontweight="bold")
    axes[1, 1].set_ylabel("Melhoria (%)")
    axes[1, 1].set_xlabel("Vers√µes do Modelo")
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    metrics_path = artifacts_dir / "metrics_history.png"
    fig.autofmt_xdate()
    fig.tight_layout()
    fig.savefig(metrics_path, dpi=150, bbox_inches="tight")
    plt.close()
    
    print(f"\nüìä Gr√°fico salvo: {colored(str(metrics_path), 'cyan')}")


def validate_model(artifact_path: str):
    """Fun√ß√£o principal de valida√ß√£o."""
    print_header("üîç VALIDA√á√ÉO DE MODELO - AN√ÅLISE COMPLETA")
    print(f"\nArquivo: {colored(artifact_path, 'yellow')}")
    
    # Carregar modelo
    try:
        artifact = load_model(artifact_path)
    except Exception as e:
        print(colored(f"\n‚ùå Erro ao carregar modelo: {e}", "red"))
        sys.exit(1)
    
    metadata = artifact.get("metadata", {})
    metrics = metadata.get("metrics", {})
    
    # Verificar integridade
    if "model" not in artifact:
        print(colored("\n‚ö†Ô∏è  AVISO: Chave 'model' n√£o encontrada no artifact!", "yellow"))
    
    if not metadata:
        print(colored("\n‚ö†Ô∏è  AVISO: Metadados vazios ou ausentes!", "yellow"))
    
    # M√©tricas de Desempenho
    print_header("üìä M√âTRICAS DE DESEMPENHO")
    
    roc_auc = metrics.get("roc_auc", 0) or 0.0
    roc_interp, roc_color = interpret_roc_auc(roc_auc)
    print_metric(
        "ROC-AUC Score",
        roc_auc,
        roc_interp,
        roc_color,
        "Mede capacidade de discrimina√ß√£o (0.5=aleat√≥rio, 1.0=perfeito)"
    )
    
    brier = metrics.get("brier_score", 1) or 1.0
    brier_interp, brier_color = interpret_brier(brier)
    print_metric(
        "Brier Score",
        brier,
        brier_interp,
        brier_color,
        "Mede calibra√ß√£o das probabilidades (0.0=perfeito, 0.25=aleat√≥rio)"
    )
    
    # Avalia√ß√£o Geral
    print_header("‚úÖ AVALIA√á√ÉO GERAL DO MODELO")
    
    scores = {
        "EXCEPCIONAL": 5,
        "EXCELENTE": 4,
        "BOM": 3,
        "RAZO√ÅVEL": 2,
        "FRACO": 1,
        "MUITO FRACO": 0
    }
    avg_score = (scores.get(roc_interp, 0) + scores.get(brier_interp, 0)) / 2
    
    if avg_score >= 4.5:
        print(colored("\nüèÜ MODELO EXCEPCIONAL - PRONTO PARA PRODU√á√ÉO", "green", attrs=["bold"]))
        print("   ‚úì M√©tricas excelentes em todos os aspectos")
        print("   ‚úì Alta confiabilidade para uso em produ√ß√£o")
        print("   ‚úì Pode ser usado para decis√µes cr√≠ticas")
    elif avg_score >= 3.5:
        print(colored("\n‚úÖ MODELO EXCELENTE - PRONTO PARA PRODU√á√ÉO", "green", attrs=["bold"]))
        print("   ‚úì M√©tricas muito boas")
        print("   ‚úì Apto para uso em produ√ß√£o")
        print("   ‚úì Monitoramento regular recomendado")
    elif avg_score >= 2.5:
        print(colored("\n‚úì MODELO UTILIZ√ÅVEL - COM RESSALVAS", "blue", attrs=["bold"]))
        print("   ‚Ä¢ Desempenho aceit√°vel para uso n√£o-cr√≠tico")
        print("   ‚Ä¢ Recomenda-se melhorias antes de produ√ß√£o")
        print("   ‚Ä¢ Use com supervis√£o humana para decis√µes importantes")
    elif avg_score >= 1.5:
        print(colored("\n‚ö†Ô∏è  MODELO PRECISA MELHORAR", "yellow", attrs=["bold"]))
        print("   ‚Ä¢ N√£o recomendado para produ√ß√£o")
        print("   ‚Ä¢ Retreinar com mais dados ou ajustar estrat√©gia")
        print("   ‚Ä¢ Considere revis√£o completa do pipeline")
    else:
        print(colored("\n‚ùå MODELO INADEQUADO", "red", attrs=["bold"]))
        print("   ‚Ä¢ N√ÉO usar em produ√ß√£o")
        print("   ‚Ä¢ Pouco ou nenhum poder preditivo")
        print("   ‚Ä¢ Revisar completamente dados e abordagem")
    
    # Compara√ß√£o com Baseline
    compare_with_baseline(metadata)
    
    # Informa√ß√µes do Dataset
    print_header("üìÅ INFORMA√á√ïES DO DATASET E TREINAMENTO")
    
    n_samples = metadata.get("n_samples", 0)
    n_features = metadata.get("n_features", 0)
    cv_splits = metadata.get("cv_splits", 0)
    model_type = metadata.get("model_type", "N/A")
    timestamp = metadata.get("timestamp", "N/A")
    
    print(f"\n{'Amostras treinadas:':<30} {colored(f'{n_samples:,}', 'cyan')}")
    print(f"{'Features utilizadas:':<30} {colored(f'{n_features:,}', 'cyan')}")
    print(f"{'Cross-validation splits:':<30} {colored(cv_splits, 'cyan')}")
    print(f"{'Tipo de modelo:':<30} {colored(model_type.upper(), 'cyan')}")
    
    if timestamp != "N/A":
        print(f"{'Data de treinamento:':<30} {colored(timestamp[:19].replace('T', ' '), 'cyan')}")
    
    # M√©tricas adicionais
    additional = calculate_additional_metrics(metadata)
    if additional:
        samples_per_feature = additional.get("samples_per_feature", 0)
        colored_value = colored(f"{samples_per_feature:.1f}", "cyan")
        print(f"\n{'Amostras por feature:':<30} {colored_value}")
        suff = additional.get("data_sufficiency", "N/A")
        suff_color = additional.get("sufficiency_color", "white")
        print(f"{'Sufici√™ncia de dados:':<30} {colored(suff, suff_color, attrs=['bold'])}")
        
        if suff == "INSUFICIENTE":
            print(colored("   ‚ö†Ô∏è  Regra de ouro: 10+ amostras por feature", "yellow"))
    
    # Mapeamento de classes
    class_mapping = metadata.get("class_mapping", {})
    if class_mapping:
        print(f"\n{'Mapeamento de Classes:':<30}")
        negative = class_mapping.get("negative", [])
        positive = class_mapping.get("positive", [])
        print(f"  {'‚Ä¢ Classe 0 (Negativa):':<28} {', '.join(negative) if negative else 'N/A'}")
        print(f"  {'‚Ä¢ Classe 1 (Positiva):':<28} {', '.join(positive) if positive else 'N/A'}")
    
    # Hiperpar√¢metros (se dispon√≠vel)
    hyperparams = metadata.get("hyperparameters", {})
    best_params = metadata.get("best_params", {})
    
    if best_params:
        print(f"\n{'Hiperpar√¢metros Otimizados:':<30}")
        for key, value in best_params.items():
            print(f"  ‚Ä¢ {key:<26} {value}")
    elif hyperparams and len(hyperparams) <= 10:
        print(f"\n{'Hiperpar√¢metros:':<30}")
        for key, value in list(hyperparams.items())[:10]:
            print(f"  ‚Ä¢ {key:<26} {value}")
    
    # Recomenda√ß√µes
    recommendations = get_recommendations(metadata)
    if recommendations:
        print_header("üí° RECOMENDA√á√ïES DE MELHORIA")
        for i, rec in enumerate(recommendations, 1):
            print(f"\n{i}. {rec}")
    else:
        print_header("üí° RECOMENDA√á√ïES")
        print("\n‚úÖ Modelo est√° em √≥timas condi√ß√µes!")
        print("   ‚Ä¢ Continue monitorando performance em produ√ß√£o")
        print("   ‚Ä¢ Retreine periodicamente com novos dados")
    
    # Hist√≥rico de m√©tricas
    artifacts_dir = Path(artifact_path).parent
    if HAS_MATPLOTLIB:
        try:
            plot_metrics_history(artifacts_dir)
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Erro ao gerar gr√°ficos: {e}")
    else:
        print(f"\nüí° Instale matplotlib e seaborn para visualiza√ß√µes:")
        print(f"   {colored('pip install matplotlib seaborn', 'yellow')}")
    
    # Pr√≥ximos passos
    print_header("üöÄ PR√ìXIMOS PASSOS RECOMENDADOS")
    
    if avg_score >= 3.5:
        print("\n1. ‚úÖ Testar com dados de valida√ß√£o externos:")
        print(f"   {colored('python test_predictions.py', 'green')}")
        
        print("\n2. üìä Analisar predi√ß√µes individuais:")
        print(f"   {colored('from src.inference import load_model, predict_proba', 'green')}")
        
        print("\n3. üöÄ Preparar para deploy:")
        print("   ‚Ä¢ Documentar vers√£o e m√©tricas")
        print("   ‚Ä¢ Configurar monitoramento de drift")
        print("   ‚Ä¢ Estabelecer pipeline de retreinamento")
    else:
        print("\n1. üîß Melhorar o modelo:")
        print("   ‚Ä¢ Revisar feature engineering")
        print("   ‚Ä¢ Coletar mais dados de qualidade")
        print("   ‚Ä¢ Experimentar diferentes algoritmos")
        
        print("\n2. üìä An√°lise explorat√≥ria:")
        print("   ‚Ä¢ Verificar distribui√ß√£o de classes")
        print("   ‚Ä¢ Analisar correla√ß√µes features vs target")
        print("   ‚Ä¢ Identificar outliers")
    
    print("\n3. üìà Monitoramento cont√≠nuo:")
    print("   ‚Ä¢ Calcular m√©tricas em dados novos mensalmente")
    print("   ‚Ä¢ Verificar feature drift")
    print("   ‚Ä¢ Retreinar quando performance degradar")
    print("   ‚Ä¢ Manter hist√≥rico de vers√µes\n")
    
    # Sum√°rio final
    print("=" * 80)
    print(colored("SUM√ÅRIO:", "cyan", attrs=["bold"]))
    print(f"  ROC-AUC: {colored(f'{roc_auc:.4f}', roc_color)} ({roc_interp})")
    print(f"  Brier:   {colored(f'{brier:.4f}', brier_color)} ({brier_interp})")
    print(f"  Status:  ", end="")
    if avg_score >= 3.5:
        print(colored("APROVADO PARA PRODU√á√ÉO ‚úì", "green", attrs=["bold"]))
    elif avg_score >= 2.5:
        print(colored("UTILIZ√ÅVEL COM RESSALVAS ‚ö†", "yellow", attrs=["bold"]))
    else:
        print(colored("REQUER MELHORIAS ‚úó", "red", attrs=["bold"]))
    print("=" * 80 + "\n")


if __name__ == "__main__":
    project_root = Path(__file__).resolve().parents[1]
    
    if len(sys.argv) < 2:
        # Buscar modelo mais recente
        artifacts_dir = project_root / "artifacts"
        if artifacts_dir.exists():
            models = list(artifacts_dir.glob("*.pkl"))
            if models:
                latest = max(models, key=lambda p: p.stat().st_mtime)
                print(colored(
                    f"Modelo mais recente encontrado: {latest.name}",
                    "yellow",
                    attrs=["bold"],
                ))
                validate_model(latest)
            else:
                print(colored(
                    "Nenhum modelo encontrado na pasta artifacts.",
                    "yellow",
                    attrs=["bold"],
                ))
        else:
            print(colored(
                "Pasta artifacts n√£o encontrada. Crie-a primeiro.",
                "yellow",
                attrs=["bold"],
            ))
    else:
        model_path = Path(sys.argv[1])
        if model_path.exists():
            validate_model(model_path)
        else:
            print(colored(
                f"Modelo {model_path.name} n√£o encontrado.",
                "red",
                attrs=["bold"],
            ))
